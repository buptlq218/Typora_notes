# **文献阅读**

## 1.EarthGPT: A Universal Multimodal Large Language
Model for Multisensor Image Comprehension in
Remote Sensing Domain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024

遥感+多模态大模型MLLMs

### **MLLMs核心架构：**

1.视觉编码器：即擅长全局上下文建模提取空间结构的vit+擅长捕捉局部细节提取纹理边缘的CNN组成的视觉增强感知机制。提升图像的细粒度与粗粒度特征。

2.语言编码器：即将用户指令与问题等文本转为语言向量，供理解复杂语义

3.跨模态对齐模块：将图像特征与语言特征拼接成统一序列输入大模型。使用解冻部分自注意力和归一化层（避免全模型微调的计算成本），使语言模型可以识图。

### **具体任务评估：**

- 场景分类
- 图像描述
- 视觉问答
- 视觉定位
- 目标检测

多轮任务对话：相比传统模型可以通过自然语言“交互式逐步引导”模型完成多任务。

- 场景识别
- 生成图像描述 image captioning
- 回答用户提问
- 检测图中物体并描述
- 定位特定目标VG
- 对特定区域生成描述

***

### **图文对齐的关键技术点：**

1.视觉特征投影：cnn vit输出的是高维图像张量，需要通过线性层降维成语言模型可接受的1D向量

2.语言嵌入：文本通过分词+向量化，转换为嵌入向量

3.多模态融合输入：将视觉token与语言token拼接，作为LLM输入

4.部分参数解冻训练：仅解冻LLM中的关键参数（如注意力头、RMSNorm），避免训练开销过大

5.损失函数：使用语言建模任务中的“自回归损失”优化整体对齐能力

***

### **创新与核心内容**

1.提出一个统一架构的遥感多模态大模型earthgpt，可统一处理分类、图像描述、视觉问答、目标检测与视觉定位等任务。并可适应多模态输入如sar合成孔径雷达图像、红外与光学图像等。

2.关键技术组件：

- 融合了CNN与Vit提取全局与局部的特征，增强了遥感图像理解能力
- 跨模态相互理解机制：通过联合训练视觉与语言表示，实现图文对齐与深层交互理解
- 统一指令微调方法：通过大规模指令跟随数据集训练，实现模型跨任务理解和语言驱动的任务执行能力。

3.数据集构建：图文对of 遥感数据集

构建与作用：

- 现有遥感数据集大多是“单任务，单模态”，如只有分类标签或者目标框。而没有统一格式的数据，难以训练多任务多模态的MLLM。

数据构建方法与格式转化：

粗粒度任务（图像级）构建方式：

- 分类任务：转化为问题“这张图属于哪个类别？”
- 图像描述：“请用一句话描述图像”
- 视觉问答VQA：保留问答对，统一格式“请用一个词或者短语回答”

细粒度任务（区域级）构建方式：

- 目标检测：转换为“请标出图中所有目标，使用HBB水平边界框 OBB方向的边界框表达”
- 视觉定位VG：用户描述一个目标“模型返回区域坐标”，或用户提供区域坐标“模型生成该区域的文本描述”

4.广泛评估与实验验证：展示了在监督与零样本场景下的卓越泛化能力。

具体表现为两阶段策略：

第一阶段：跨模态对齐，数据来源是自然图像数据集。是为了对语言模型的关键层解冻，进行图文联合训练，即依托于自然图像的数据量与多样性大，可用于“教模型看图说话；

第二阶段：遥感图像视觉模式不同、任务语义专业，必须单独训练模型适应。在遥感领域微调可以让模型掌握遥感任务与语境，采用轻量的bias-tunning方法，在遥感任务中精调模型指令响应能力。

**这么做的原因：直接在遥感数据训练模型难以对图文理解进行有效初始化，因此需要先在通用数据上预训练，然后迁移到遥感。**两步培训中，采用了域转移学习策略将通用知识转移到RS领域。

==指令驱动学习：数据本身构成指令跟随样本，为语言引导任务执行提供基础。==

